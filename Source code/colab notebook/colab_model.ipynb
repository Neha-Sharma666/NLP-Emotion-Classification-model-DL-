{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMlvd6kMTERW"
      },
      "source": [
        "Early-Stopping Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUsAKazhTXX0"
      },
      "source": [
        "Importing Necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikqIMZHktu34"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GibGq7csBuer"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('outcclean.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Doqlmye1Tmis"
      },
      "source": [
        "Splitting Data->Train,Validate,Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ueqk4i_ubVt"
      },
      "outputs": [],
      "source": [
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df['text'].values,\n",
        "    df['emotion'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts,\n",
        "    temp_labels,\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqoGZYn-nDlR"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of training samples: {len(train_texts)}\")\n",
        "print(f\"Number of validation samples: {len(val_texts)}\")\n",
        "print(f\"Number of test samples: {len(test_texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Pe4LWzUz0G"
      },
      "source": [
        "Label Encoding For Emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOPOf3qSuu-a"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "val_labels = label_encoder.transform(val_labels)\n",
        "test_labels = label_encoder.transform(test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPnxdZcBiGYK"
      },
      "outputs": [],
      "source": [
        "df['emotion'] = label_encoder.fit_transform(df['emotion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao8G1uK_jHYc"
      },
      "outputs": [],
      "source": [
        "# Get the mapping of encoded labels to original emotions\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "print(\"Label Encoding Mapping:\")\n",
        "for emotion, label in label_mapping.items():\n",
        "    print(f\"{emotion}: {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qWed8V3gKrp"
      },
      "outputs": [],
      "source": [
        "train_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgw5i7ZqVCur"
      },
      "source": [
        "Loading and Initialization of Indicbert Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOA-RKndu2lR"
      },
      "outputs": [],
      "source": [
        "model_name = \"ai4bharat/indic-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, keep_accents=True)\n",
        "model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "model.config.output_hidden_states = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I3zTN5zhQQV"
      },
      "outputs": [],
      "source": [
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z2aaWMPhrw4"
      },
      "outputs": [],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAIVi6spWcaW"
      },
      "source": [
        "Defining a Custom Dataset Class And Generating Sentence Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKYl2M_J1OH5"
      },
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, model, device):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize and encode text\n",
        "        encoding = self.tokenizer(text, return_tensors='pt', truncation=True, padding='max_length',  max_length=128)\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # Move tensors to device\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "\n",
        "        # Forward pass through model to get sentence embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
        "        hidden_states = outputs.hidden_states\n",
        "        second_to_last_layer = hidden_states[-2]\n",
        "        sentence_embedding = second_to_last_layer.squeeze()\n",
        "\n",
        "        #Return Processed Data\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(label, dtype=torch.long, device=self.device),\n",
        "            'embedding': sentence_embedding\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ6ZZSFRYTfb"
      },
      "source": [
        "Creating Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zLSeykuu-5F"
      },
      "outputs": [],
      "source": [
        "#creating data instances\n",
        "train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, model, device)\n",
        "val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, model, device)\n",
        "test_dataset = EmotionDataset(test_texts, test_labels, tokenizer, model, device)\n",
        "\n",
        "batch_size = 18\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p34UxJudghfk"
      },
      "outputs": [],
      "source": [
        "train_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xyF7nhyYp73"
      },
      "source": [
        "Defining the Model Architecture(structure and forward pass of neural network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UOmyVk6ZHBs"
      },
      "outputs": [],
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_prob=0.5):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.ln1 = nn.LayerNorm(hidden_dim1)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.ln2 = nn.LayerNorm(hidden_dim2)\n",
        "        self.relu2 = nn.ELU()\n",
        "        self.dropout2 = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.ln1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = self.ln2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.dropout2(out)\n",
        "\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk46FW1wZtDj"
      },
      "source": [
        "Setting the dimensions and hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "875AV_wcZa5e"
      },
      "outputs": [],
      "source": [
        "input_dim = model.config.hidden_size\n",
        "hidden_dim1 = 256\n",
        "hidden_dim2 = 128\n",
        "output_dim = len(label_encoder.classes_)\n",
        "dropout_prob = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLcbOjVLaYi9"
      },
      "outputs": [],
      "source": [
        "mlp_model = MLPClassifier(input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_prob).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5l2BoqoaqDu"
      },
      "source": [
        "Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM1hlnQ3vJ0z"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtDoBVuucB97"
      },
      "source": [
        "Training Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73GtknV4an8l"
      },
      "outputs": [],
      "source": [
        "num_epochs = 18\n",
        "best_val_accuracy = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtGQ0V5_cYti"
      },
      "source": [
        "Training Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cKri50_svM3W"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "for epoch in range(num_epochs):\n",
        "    mlp_model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        embeddings = batch['embedding'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        print(f\"Original embeddings shape: {embeddings.shape}\")\n",
        "        embeddings = torch.mean(embeddings, dim=1)\n",
        "#Foeward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp_model(embeddings)\n",
        "\n",
        "        print(f\"Outputs shape: {outputs.shape}\")\n",
        "        print(f\"Labels shape: {labels.shape}\")\n",
        "#Loss calculation and backpropagation\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "#calculating average training loss\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5I3H4NFdtfy"
      },
      "source": [
        "Evaluation of Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv0j00C2k3oM"
      },
      "outputs": [],
      "source": [
        "mlp_model.eval()\n",
        "val_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        embeddings = batch['embedding'].to(device)  # Shape: [batch_size, 128, 768]\n",
        "        labels = batch['labels'].to(device)  # Shape: [batch_size]\n",
        "\n",
        "        # Average over the sequence length dimension (128)\n",
        "        embeddings = torch.mean(embeddings, dim=1)  # Shape: [batch_size, 768]\n",
        "\n",
        "        outputs = mlp_model(embeddings)  # Shape: [batch_size, 4]\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "# Calculating accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "avg_val_loss = val_loss / len(val_loader)\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Validation Loss: {avg_val_loss}, Accuracy: {accuracy}%\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXoKyPuJeGsc"
      },
      "source": [
        "Evaluation of model->Accuracy & F1-Score of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMmZNqwqUyEn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def evaluate_model(model, criterion, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            embeddings = batch['embedding'].to(device)  # Shape: [batch_size, 128, 768]\n",
        "            labels = batch['labels'].to(device)  # Shape: [batch_size]\n",
        "\n",
        "            # Average over the sequence length dimension (128)\n",
        "            embeddings = torch.mean(embeddings, dim=1)  # Shape: [batch_size, 768]\n",
        "\n",
        "            outputs = model(embeddings)  # Shape: [batch_size, 4]\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Collect all labels and predictions for F1 score calculation\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "# Define your criterion (loss function) and device (CPU or GPU)\n",
        "criterion = nn.CrossEntropyLoss()  # Assuming you're using CrossEntropyLoss\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Evaluate on validation dataset\n",
        "val_loss, val_accuracy, val_f1 = evaluate_model(mlp_model, criterion, val_loader, device)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%, Validation F1 Score: {val_f1}\")\n",
        "\n",
        "# Evaluate on test dataset\n",
        "test_loss, test_accuracy, test_f1 = evaluate_model(mlp_model, criterion, test_loader, device)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}%, Test F1 Score: {test_f1}\")\n",
        "\n",
        "# Evaluate on train dataset\n",
        "train_loss, train_accuracy, train_f1 = evaluate_model(mlp_model, criterion, train_loader, device)\n",
        "print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}%, Train F1 Score: {train_f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSMwY32eUyK"
      },
      "source": [
        "Evaluation of model performance per Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "mlp_model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        embeddings = batch['embedding'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        embeddings = torch.mean(embeddings, dim=1)\n",
        "        outputs = mlp_model(embeddings)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BVCSaSbOW7BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avDCeF8m90bN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Assuming all_labels and all_preds are defined\n",
        "target_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6']\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(all_labels, all_preds, target_names=target_names)\n",
        "print(report)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Calculate accuracy for each class\n",
        "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "# Print accuracy for each class\n",
        "for i, accuracy in enumerate(class_accuracies):\n",
        "    print(f\"Accuracy of {target_names[i]}: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzqAapicA3Er"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}